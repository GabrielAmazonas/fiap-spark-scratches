{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import lower, col, lit, regexp_replace, trim, substring, when, expr, udf, count\n",
    "from pyspark.sql.types import IntegerType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_after_hiphen(team_name):\n",
    "    if team_name.startswith('atletico') or team_name.startswith('atl')  or team_name.startswith('Atl') or team_name.startswith('Ath'):\n",
    "        return team_name\n",
    "    else:\n",
    "        return team_name.split('-', 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como capturar os dados direto do Cartola FC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programaticamente, através de requisições HTTP, conforme exemplo abaixo.\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.cartolafc.globo.com/partidas/1\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('test_cartola_outputfile.json', 'wb') as outf:\n",
    "    outf.write(response.content)\n",
    "\n",
    "data = json.load(open('test_cartola_outputfile.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e84b0e8551a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#df = sqlContext.read.json(json.dumps(data))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#df = sqlContext.read.json(json.dumps(data))\n",
    "\n",
    "df = spark.read.json(sc.parallelize([data]))\n",
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HiveContext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-59ed5b4d8322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHiveContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'HiveContext' is not defined"
     ]
    }
   ],
   "source": [
    "sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ea83332336b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colocando os arquivos da pasta cartola no HDFS:\n",
    "### hadoop fs -put /cartola/* /cartola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validando os arquivos csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,StringType,true),StructField(game,StringType,true),StructField(round,StringType,true),StructField(date,StringType,true),StructField(home_team,StringType,true),StructField(score,StringType,true),StructField(away_team,StringType,true),StructField(arena,StringType,true),StructField(X,StringType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partidas_2014_df = spark.read.csv(\"/cartola/data/2014/2014_partidas.csv\", header=True)\n",
    "partidas_2014_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+------------------+---------------+-----+----------------+--------------------+----+\n",
      "|_c0|game|round|              date|      home_team|score|       away_team|               arena|   X|\n",
      "+---+----+-----+------------------+---------------+-----+----------------+--------------------+----+\n",
      "|  1|   1|    1|20/04/2014 - 18:30|  Flamengo - RJ|0 x 0|      Goiás - GO|Mané Garrincha - ...|null|\n",
      "|  2|   2|    1|19/04/2014 - 18:30|Fluminense - RJ|3 x 0|Figueirense - SC|Maracanã - Rio de...|null|\n",
      "|  3|   3|    1|20/04/2014 - 16:00| São Paulo - SP|3 x 0|   Botafogo - RJ|Morumbi - Sao Pau...|null|\n",
      "|  4|   4|    1|20/04/2014 - 18:30|    Santos - SP|1 x 1|      Sport - PE|Vila Belmiro - Sa...|null|\n",
      "|  5|   5|    1|20/04/2014 - 16:00|  Atletico - PR|1 x 0|     Grêmio - RS|Orlando Scarpelli...|null|\n",
      "+---+----+-----+------------------+---------------+-----+----------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partidas_2014_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_hiphen_udf = udf(remove_after_hiphen, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>game</th>\n",
       "      <th>round</th>\n",
       "      <th>date</th>\n",
       "      <th>home_team</th>\n",
       "      <th>score</th>\n",
       "      <th>away_team</th>\n",
       "      <th>arena</th>\n",
       "      <th>X</th>\n",
       "      <th>home_score</th>\n",
       "      <th>away_score</th>\n",
       "      <th>total_gols</th>\n",
       "      <th>ano</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20/04/2014 - 18:30</td>\n",
       "      <td>flamengo</td>\n",
       "      <td>0 x 0</td>\n",
       "      <td>goiás</td>\n",
       "      <td>Mané Garrincha - Brasilia - DF</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>empate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19/04/2014 - 18:30</td>\n",
       "      <td>fluminense</td>\n",
       "      <td>3 x 0</td>\n",
       "      <td>figueirense</td>\n",
       "      <td>Maracanã - Rio de Janeiro - RJ</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>fluminense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20/04/2014 - 16:00</td>\n",
       "      <td>são paulo</td>\n",
       "      <td>3 x 0</td>\n",
       "      <td>botafogo</td>\n",
       "      <td>Morumbi - Sao Paulo - SP</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>são paulo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20/04/2014 - 18:30</td>\n",
       "      <td>santos</td>\n",
       "      <td>1 x 1</td>\n",
       "      <td>sport</td>\n",
       "      <td>Vila Belmiro - Santos - SP</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>empate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20/04/2014 - 16:00</td>\n",
       "      <td>atletico - pr</td>\n",
       "      <td>1 x 0</td>\n",
       "      <td>grêmio</td>\n",
       "      <td>Orlando Scarpelli - Florianopolis - SC</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>atletico - pr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>376</td>\n",
       "      <td>376</td>\n",
       "      <td>38</td>\n",
       "      <td>07/12/2014 - 17:00</td>\n",
       "      <td>cruzeiro</td>\n",
       "      <td>2 x 1</td>\n",
       "      <td>fluminense</td>\n",
       "      <td>Mineirão - Belo Horizonte - MG</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>cruzeiro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>377</td>\n",
       "      <td>377</td>\n",
       "      <td>38</td>\n",
       "      <td>07/12/2014 - 17:00</td>\n",
       "      <td>vitória</td>\n",
       "      <td>0 x 1</td>\n",
       "      <td>santos</td>\n",
       "      <td>Manoel Barradas - Salvador - BA</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2014</td>\n",
       "      <td>santos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>378</td>\n",
       "      <td>378</td>\n",
       "      <td>38</td>\n",
       "      <td>07/12/2014 - 17:00</td>\n",
       "      <td>grêmio</td>\n",
       "      <td>1 x 1</td>\n",
       "      <td>flamengo</td>\n",
       "      <td>Arena do Grêmio - Porto Alegre - RS</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2014</td>\n",
       "      <td>empate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>379</td>\n",
       "      <td>379</td>\n",
       "      <td>38</td>\n",
       "      <td>06/12/2014 - 16:30</td>\n",
       "      <td>figueirense</td>\n",
       "      <td>1 x 2</td>\n",
       "      <td>internacional</td>\n",
       "      <td>Orlando Scarpelli - Florianopolis - SC</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2014</td>\n",
       "      <td>internacional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>380</td>\n",
       "      <td>380</td>\n",
       "      <td>38</td>\n",
       "      <td>07/12/2014 - 17:00</td>\n",
       "      <td>goiás</td>\n",
       "      <td>4 x 2</td>\n",
       "      <td>chapecoense</td>\n",
       "      <td>Serra Dourada - Goiania - GO</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2014</td>\n",
       "      <td>goiás</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     _c0 game round                date      home_team  score      away_team  \\\n",
       "0      1    1     1  20/04/2014 - 18:30       flamengo  0 x 0          goiás   \n",
       "1      2    2     1  19/04/2014 - 18:30     fluminense  3 x 0    figueirense   \n",
       "2      3    3     1  20/04/2014 - 16:00      são paulo  3 x 0       botafogo   \n",
       "3      4    4     1  20/04/2014 - 18:30         santos  1 x 1          sport   \n",
       "4      5    5     1  20/04/2014 - 16:00  atletico - pr  1 x 0         grêmio   \n",
       "..   ...  ...   ...                 ...            ...    ...            ...   \n",
       "375  376  376    38  07/12/2014 - 17:00       cruzeiro  2 x 1     fluminense   \n",
       "376  377  377    38  07/12/2014 - 17:00        vitória  0 x 1         santos   \n",
       "377  378  378    38  07/12/2014 - 17:00         grêmio  1 x 1       flamengo   \n",
       "378  379  379    38  06/12/2014 - 16:30    figueirense  1 x 2  internacional   \n",
       "379  380  380    38  07/12/2014 - 17:00          goiás  4 x 2    chapecoense   \n",
       "\n",
       "                                      arena     X  home_score  away_score  \\\n",
       "0            Mané Garrincha - Brasilia - DF  None           0           0   \n",
       "1            Maracanã - Rio de Janeiro - RJ  None           3           0   \n",
       "2                  Morumbi - Sao Paulo - SP  None           3           0   \n",
       "3                Vila Belmiro - Santos - SP  None           1           1   \n",
       "4    Orlando Scarpelli - Florianopolis - SC  None           1           0   \n",
       "..                                      ...   ...         ...         ...   \n",
       "375          Mineirão - Belo Horizonte - MG  None           2           1   \n",
       "376         Manoel Barradas - Salvador - BA  None           0           1   \n",
       "377     Arena do Grêmio - Porto Alegre - RS  None           1           1   \n",
       "378  Orlando Scarpelli - Florianopolis - SC  None           1           2   \n",
       "379            Serra Dourada - Goiania - GO  None           4           2   \n",
       "\n",
       "     total_gols   ano         result  \n",
       "0             0  2014         empate  \n",
       "1             3  2014     fluminense  \n",
       "2             3  2014      são paulo  \n",
       "3             2  2014         empate  \n",
       "4             1  2014  atletico - pr  \n",
       "..          ...   ...            ...  \n",
       "375           3  2014       cruzeiro  \n",
       "376           1  2014         santos  \n",
       "377           2  2014         empate  \n",
       "378           3  2014  internacional  \n",
       "379           6  2014          goiás  \n",
       "\n",
       "[380 rows x 14 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criar nome do time com a string antes do Hífen\n",
    "partidas_2014_ct = partidas_2014_df.withColumn('away_team', remove_hiphen_udf(partidas_2014_df['away_team']))\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('away_team', lower(col('away_team')))\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('away_team', trim(col('away_team')))\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('home_team', remove_hiphen_udf(partidas_2014_ct['home_team']))\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('home_team', lower(col('home_team')))\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('home_team', trim(col('home_team')))\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('home_score', substring('score', 1,1))\\\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('away_score', substring('score', 5, 5))\\\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('home_score', partidas_2014_ct['home_score'].cast(IntegerType()))\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('away_score', partidas_2014_ct['away_score'].cast(IntegerType()))\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('total_gols', partidas_2014_ct['away_score'] + partidas_2014_ct['home_score'] )\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('ano', lit(2014))\n",
    "\n",
    "time_ganhador = expr(\n",
    "    \"\"\"IF(home_score > away_score, home_team, IF(home_score = away_score, 'empate', away_team))\"\"\"\n",
    ")\n",
    "\n",
    "partidas_2014_ct = partidas_2014_ct.withColumn('result', time_ganhador)\n",
    "\n",
    "partidas_2014_ct.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantas partidas resultaram em empate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|       result|total_wins|\n",
      "+-------------+----------+\n",
      "|       empate|        92|\n",
      "|     cruzeiro|        24|\n",
      "|internacional|        21|\n",
      "|    são paulo|        20|\n",
      "|  corinthians|        19|\n",
      "+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -        Quantas partidas resultaram em empate?\n",
    "count_result = partidas_2014_ct.groupBy(\"result\").agg(count(\"*\")).orderBy(\"count(1)\")\n",
    "\n",
    "count_result = count_result.withColumn('total_wins', count_result['count(1)'])\n",
    "\n",
    "count_result = count_result.sort(count_result.total_wins.desc())\n",
    "\n",
    "count_result = count_result.drop('count(1)')\n",
    "\n",
    "count_result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_lances_2014_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-874503e735c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted_lances_2014_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_lances_2014_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_lances_2014_df' is not defined"
     ]
    }
   ],
   "source": [
    "sorted_lances_2014_df.show(5)\n",
    "\n",
    "print(sorted_lances_2014_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo todas as partidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_c0,StringType,true),StructField(game,StringType,true),StructField(round,StringType,true),StructField(date,StringType,true),StructField(home_team,StringType,true),StructField(score,StringType,true),StructField(away_team,StringType,true),StructField(arena,StringType,true),StructField(X,StringType,true)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tds_partidas_df = spark.read.csv(\"/cartola/data/*/*_partidas.csv\", header=True)\n",
    "\n",
    "tds_partidas_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------------+------------------+----------------+-------------+--------------------+--------------------+----+\n",
      "|_c0|game|             round|              date|       home_team|        score|           away_team|               arena|   X|\n",
      "+---+----+------------------+------------------+----------------+-------------+--------------------+--------------------+----+\n",
      "| 99|  99|                10|16/07/2014 - 22:00|   Criciuma - SC|        3 x 2|     Fluminense - RJ|Heriberto Hulse -...|null|\n",
      "| 99|  99|                10|02/07/2015 - 19:30|Figueirense - SC|        3 x 1|          Goiás - GO|Orlando Scarpelli...|null|\n",
      "| 99|  99|                10|25/06/2017 - 16:00|Ponte Preta - SP|        1 x 2|      Palmeiras - SP|Moisés Lucarelli ...|null|\n",
      "| 99|  99|                10|23/06/2016 - 21:00|  São Paulo - SP|        0 x 0|          Sport - PE|Morumbi - Sao Pau...|null|\n",
      "| 99|  10|06/06/2018 - 21:00|  Corinthians - SP|           1 x 1|  Santos - SP|Arena Corinthians...|                null|null|\n",
      "| 98|  98|                10|02/07/2015 - 19:30|Corinthians - SP|        2 x 0|    Ponte Preta - SP|Arena Corinthians...|null|\n",
      "| 98|  10|06/06/2018 - 21:00|        Sport - PE|           1 x 0|Atlético - PR|Ilha do Retiro - ...|                null|null|\n",
      "| 98|  98|                10|16/07/2014 - 19:30|     Grêmio - RS|        0 x 0|          Goiás - GO|Arena do Grêmio -...|null|\n",
      "| 98|  98|                10|23/06/2016 - 19:15|     Grêmio - RS|        1 x 2|        Vitória - BA|Arena do Grêmio -...|null|\n",
      "| 98|  98|                10|25/06/2017 - 18:30|      Bahia - BA|        0 x 1|       Flamengo - RJ|Arena Fonte Nova ...|null|\n",
      "+---+----+------------------+------------------+----------------+-------------+--------------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2670"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc_sorted_partidas_df = tds_partidas_df.sort(tds_partidas_df._c0.desc())\n",
    "\n",
    "desc_sorted_partidas_df.show(10)\n",
    "\n",
    "desc_sorted_partidas_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------------+------------------+------------------+-----------+--------------------+--------------------+----+\n",
      "|_c0|game|             round|              date|         home_team|      score|           away_team|               arena|   X|\n",
      "+---+----+------------------+------------------+------------------+-----------+--------------------+--------------------+----+\n",
      "|  1|   1|                 1|14/05/2017 - 11:00|   Fluminense - RJ|      3 x 2|         Santos - SP|Maracanã - Rio de...|null|\n",
      "|  1|   1|                 1|14/05/2016 - 16:00|    Palmeiras - SP|      4 x 0|       Atlético - PR|Allianz Parque - ...|null|\n",
      "|  1|   1|                 1|09/05/2015 - 18:30|    Palmeiras - SP|      2 x 2|       Atlético - MG|Allianz Parque - ...|null|\n",
      "|  1|   1|                 1|20/04/2014 - 18:30|     Flamengo - RJ|      0 x 0|          Goiás - GO|Mané Garrincha - ...|null|\n",
      "|  1|   1|14/04/2018 - 16:00|     Cruzeiro - MG|             0 x 1|Grêmio - RS|Mineirão - Belo H...|                null|null|\n",
      "| 10|  10|                 1|14/05/2017 - 16:00|         Avaí - SC|      0 x 0|        Vitória - BA|Ressacada - Flori...|null|\n",
      "| 10|   1|15/04/2018 - 16:00|Internacional - RS|             2 x 0| Bahia - BA|Beira-Rio - Porto...|                null|null|\n",
      "| 10|  10|                 1|10/05/2015 - 18:30|         Avaí - SC|      1 x 1|         Santos - SP|Ressacada - Flori...|null|\n",
      "| 10|  10|                 1|15/05/2016 - 18:30|Internacional - RS|      0 x 0|    Chapecoense - SC|Beira Rio - Porto...|null|\n",
      "| 10|  10|                 1|19/04/2014 - 21:00|  Chapecoense - SC|      0 x 0|       Coritiba - PR|Arena Condá - Cha...|null|\n",
      "+---+----+------------------+------------------+------------------+-----------+--------------------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2670"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_sorted_partidas_df = tds_partidas_df.sort(tds_partidas_df._c0.asc())\n",
    "\n",
    "asc_sorted_partidas_df.show(10)\n",
    "\n",
    "asc_sorted_partidas_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Devemos usar os dados de todos os anos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pergunta 1: Quantos registros há na tabela por ano?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2670"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Na tabela partidas:\n",
    "asc_sorted_partidas_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos contar os registros de cada uma das tabelas que serão criadas ou precisa ser uma única necessariamente?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pergunta 2: Quantas equipes únicas mandantes existem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ler tabela times\n",
    "times_df = spark.read.csv(\"/cartola/data/times_ids.csv\", header=True)\n",
    "times_df.count()\n",
    "\n",
    "# sorted_times_df = times_df.sort(times_df.id.desc())\n",
    "# sorted_times_df.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pergunta 3: Quantas vezes as equipes mandantes saíram vitoriosas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identificado arquivo csv com valores separados por \";\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert DataFrame Row Object to a new DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert spark DataFrame column of type list to new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(atletas,ArrayType(StructType(List(StructField(apelido,StringType,true),StructField(atleta_id,LongType,true),StructField(clube_id,LongType,true),StructField(foto,StringType,true),StructField(jogos_num,LongType,true),StructField(media_num,DoubleType,true),StructField(nome,StringType,true),StructField(pontos_num,DoubleType,true),StructField(posicao_id,LongType,true),StructField(preco_num,DoubleType,true),StructField(rodada_id,LongType,true),StructField(scout,StructType(List(StructField(A,LongType,true),StructField(CA,LongType,true),StructField(CV,LongType,true),StructField(DD,LongType,true),StructField(DP,LongType,true),StructField(FC,LongType,true),StructField(FD,LongType,true),StructField(FF,LongType,true),StructField(FS,LongType,true),StructField(FT,LongType,true),StructField(G,LongType,true),StructField(GC,LongType,true),StructField(GS,LongType,true),StructField(I,LongType,true),StructField(PE,LongType,true),StructField(PP,LongType,true),StructField(RB,LongType,true),StructField(SG,LongType,true))),true),StructField(status_id,LongType,true),StructField(variacao_num,DoubleType,true))),true),true),StructField(clubes,StructType(List(StructField(262,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(263,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(264,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(265,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(266,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(267,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(275,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(276,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(277,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(282,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(283,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(284,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(287,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(292,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(293,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(294,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(303,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(314,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(315,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true),StructField(373,StructType(List(StructField(abreviacao,StringType,true),StructField(escudos,StructType(List(StructField(30x30,StringType,true),StructField(45x45,StringType,true),StructField(60x60,StringType,true))),true),StructField(id,LongType,true),StructField(nome,StringType,true),StructField(posicao,LongType,true))),true))),true),StructField(posicoes,StructType(List(StructField(1,StructType(List(StructField(abreviacao,StringType,true),StructField(id,LongType,true),StructField(nome,StringType,true))),true),StructField(2,StructType(List(StructField(abreviacao,StringType,true),StructField(id,LongType,true),StructField(nome,StringType,true))),true),StructField(3,StructType(List(StructField(abreviacao,StringType,true),StructField(id,LongType,true),StructField(nome,StringType,true))),true),StructField(4,StructType(List(StructField(abreviacao,StringType,true),StructField(id,LongType,true),StructField(nome,StringType,true))),true),StructField(5,StructType(List(StructField(abreviacao,StringType,true),StructField(id,LongType,true),StructField(nome,StringType,true))),true),StructField(6,StructType(List(StructField(abreviacao,StringType,true),StructField(id,LongType,true),StructField(nome,StringType,true))),true))),true),StructField(status,StructType(List(StructField(2,StructType(List(StructField(id,LongType,true),StructField(nome,StringType,true))),true),StructField(3,StructType(List(StructField(id,LongType,true),StructField(nome,StringType,true))),true),StructField(5,StructType(List(StructField(id,LongType,true),StructField(nome,StringType,true))),true),StructField(6,StructType(List(StructField(id,LongType,true),StructField(nome,StringType,true))),true),StructField(7,StructType(List(StructField(id,LongType,true),StructField(nome,StringType,true))),true))),true)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(apelido,StringType,true),StructField(atleta_id,LongType,true),StructField(clube_id,LongType,true),StructField(foto,StringType,true),StructField(jogos_num,LongType,true),StructField(media_num,DoubleType,true),StructField(nome,StringType,true),StructField(pontos_num,DoubleType,true),StructField(posicao_id,LongType,true),StructField(preco_num,DoubleType,true),StructField(rodada_id,LongType,true),StructField(scout,StructType(List(StructField(A,LongType,true),StructField(CA,LongType,true),StructField(CV,LongType,true),StructField(DD,LongType,true),StructField(DP,LongType,true),StructField(FC,LongType,true),StructField(FD,LongType,true),StructField(FF,LongType,true),StructField(FS,LongType,true),StructField(FT,LongType,true),StructField(G,LongType,true),StructField(GC,LongType,true),StructField(GS,LongType,true),StructField(I,LongType,true),StructField(PE,LongType,true),StructField(PP,LongType,true),StructField(RB,LongType,true),StructField(SG,LongType,true))),true),StructField(status_id,LongType,true),StructField(variacao_num,DoubleType,true)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atletas_test = df.select(\"atletas\").rdd.flatMap(lambda x: x[0]).collect()\n",
    "\n",
    "\n",
    "atletas_df = spark.createDataFrame(atletas_test) \n",
    "atletas_df.head(3)\n",
    "\n",
    "# Onde o novo dataframe foi salvo?\n",
    "# atletas_df.write.json(\"teste/atletas_teste.json\")\n",
    "atletas_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "atletas_df.createOrReplaceTempView(\"tempAtletas\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# /user/hive/warehouse/teste_tabela_atletas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.sql(\"create table atletas_hive_test as select * from tempAtletas\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------+\n",
      "|database|        tableName|isTemporary|\n",
      "+--------+-----------------+-----------+\n",
      "| default|atletas_hive_test|      false|\n",
      "|        |      tempatletas|       true|\n",
      "+--------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"show tables from default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE EXTERNAL TABLE atletas (apelido STRING, atleta_id BIGINT)\n",
    "  STORED AS PARQUET LOCATION '/user/hive/warehouse/teste_tabela_atletas/';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o38.sql.\n: java.lang.NoClassDefFoundError: parquet/hadoop/ParquetOutputFormat\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:238)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$toOutputFormat(HiveClientImpl.scala:913)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$6.apply(HiveClientImpl.scala:947)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$6.apply(HiveClientImpl.scala:947)\n\tat scala.Option.map(Option.scala:146)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:947)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:278)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)\n\tat org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:128)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.GeneratedMethodAccessor85.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: parquet.hadoop.ParquetOutputFormat\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:226)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:215)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 47 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-4b5265d7ebc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sqlContext.sql(\"show databases\").show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CREATE EXTERNAL TABLE atletas (apelido STRING, atleta_id BIGINT)                STORED AS PARQUET LOCATION '/user/root/teste/atletas_teste.parquet' \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/context.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \"\"\"\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o38.sql.\n: java.lang.NoClassDefFoundError: parquet/hadoop/ParquetOutputFormat\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:238)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$toOutputFormat(HiveClientImpl.scala:913)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$6.apply(HiveClientImpl.scala:947)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$6.apply(HiveClientImpl.scala:947)\n\tat scala.Option.map(Option.scala:146)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:947)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:278)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)\n\tat org.apache.spark.sql.execution.command.CreateTableCommand.run(tables.scala:128)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:79)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:194)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:79)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.GeneratedMethodAccessor85.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: parquet.hadoop.ParquetOutputFormat\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:226)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:215)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 47 more\n"
     ]
    }
   ],
   "source": [
    "# sqlContext.sql(\"show databases\").show()\n",
    "\n",
    "sqlContext.sql(\"CREATE EXTERNAL TABLE atletas (apelido STRING, atleta_id BIGINT) \\\n",
    "               STORED AS PARQUET LOCATION '/user/root/teste/atletas_teste.parquet' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setProperty(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.sql.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = pyspark.SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f5e974946a0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.executor.uri\", \"thrift://hive-metastore:9083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o749.load.\n: java.sql.SQLException: Query failed (#20210421_010659_00001_ddua7): line 3:3: Table hive.default.pessoas does not exist\n\tat io.prestosql.jdbc.PrestoResultSet.resultsException(PrestoResultSet.java:1894)\n\tat io.prestosql.jdbc.PrestoResultSet.getColumns(PrestoResultSet.java:1755)\n\tat io.prestosql.jdbc.PrestoResultSet.<init>(PrestoResultSet.java:125)\n\tat io.prestosql.jdbc.PrestoStatement.internalExecute(PrestoStatement.java:249)\n\tat io.prestosql.jdbc.PrestoStatement.execute(PrestoStatement.java:227)\n\tat io.prestosql.jdbc.PrestoPreparedStatement.executeQuery(PrestoPreparedStatement.java:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:210)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.facebook.presto.sql.analyzer.SemanticException: line 3:3: Table hive.default.pessoas does not exist\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitTable(StatementAnalyzer.java:775)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitTable(StatementAnalyzer.java:245)\n\tat com.facebook.presto.sql.tree.Table.accept(Table.java:53)\n\tat com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:257)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.analyzeFrom(StatementAnalyzer.java:1713)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuerySpecification(StatementAnalyzer.java:877)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuerySpecification(StatementAnalyzer.java:245)\n\tat com.facebook.presto.sql.tree.QuerySpecification.accept(QuerySpecification.java:127)\n\tat com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:257)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:267)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuery(StatementAnalyzer.java:614)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuery(StatementAnalyzer.java:245)\n\tat com.facebook.presto.sql.tree.Query.accept(Query.java:94)\n\tat com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:257)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer.analyze(StatementAnalyzer.java:231)\n\tat com.facebook.presto.sql.analyzer.Analyzer.analyze(Analyzer.java:72)\n\tat com.facebook.presto.sql.analyzer.Analyzer.analyze(Analyzer.java:64)\n\tat com.facebook.presto.execution.SqlQueryExecution.doAnalyzeQuery(SqlQueryExecution.java:351)\n\tat com.facebook.presto.execution.SqlQueryExecution.analyzeQuery(SqlQueryExecution.java:337)\n\tat com.facebook.presto.execution.SqlQueryExecution.start(SqlQueryExecution.java:269)\n\tat com.facebook.presto.execution.QueuedExecution.lambda$start$1(QueuedExecution.java:62)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-11b5a8adaa97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjdbcDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jdbc\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"com.facebook.presto.jdbc.PrestoDriver\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"url\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"jdbc:presto://presto:8080/hive/default\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hive\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbtable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pessoas\"\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o749.load.\n: java.sql.SQLException: Query failed (#20210421_010659_00001_ddua7): line 3:3: Table hive.default.pessoas does not exist\n\tat io.prestosql.jdbc.PrestoResultSet.resultsException(PrestoResultSet.java:1894)\n\tat io.prestosql.jdbc.PrestoResultSet.getColumns(PrestoResultSet.java:1755)\n\tat io.prestosql.jdbc.PrestoResultSet.<init>(PrestoResultSet.java:125)\n\tat io.prestosql.jdbc.PrestoStatement.internalExecute(PrestoStatement.java:249)\n\tat io.prestosql.jdbc.PrestoStatement.execute(PrestoStatement.java:227)\n\tat io.prestosql.jdbc.PrestoPreparedStatement.executeQuery(PrestoPreparedStatement.java:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:61)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:210)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:318)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:167)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.facebook.presto.sql.analyzer.SemanticException: line 3:3: Table hive.default.pessoas does not exist\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitTable(StatementAnalyzer.java:775)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitTable(StatementAnalyzer.java:245)\n\tat com.facebook.presto.sql.tree.Table.accept(Table.java:53)\n\tat com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:257)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.analyzeFrom(StatementAnalyzer.java:1713)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuerySpecification(StatementAnalyzer.java:877)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuerySpecification(StatementAnalyzer.java:245)\n\tat com.facebook.presto.sql.tree.QuerySpecification.accept(QuerySpecification.java:127)\n\tat com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:257)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:267)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuery(StatementAnalyzer.java:614)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.visitQuery(StatementAnalyzer.java:245)\n\tat com.facebook.presto.sql.tree.Query.accept(Query.java:94)\n\tat com.facebook.presto.sql.tree.AstVisitor.process(AstVisitor.java:27)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer$Visitor.process(StatementAnalyzer.java:257)\n\tat com.facebook.presto.sql.analyzer.StatementAnalyzer.analyze(StatementAnalyzer.java:231)\n\tat com.facebook.presto.sql.analyzer.Analyzer.analyze(Analyzer.java:72)\n\tat com.facebook.presto.sql.analyzer.Analyzer.analyze(Analyzer.java:64)\n\tat com.facebook.presto.execution.SqlQueryExecution.doAnalyzeQuery(SqlQueryExecution.java:351)\n\tat com.facebook.presto.execution.SqlQueryExecution.analyzeQuery(SqlQueryExecution.java:337)\n\tat com.facebook.presto.execution.SqlQueryExecution.start(SqlQueryExecution.java:269)\n\tat com.facebook.presto.execution.QueuedExecution.lambda$start$1(QueuedExecution.java:62)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "jdbcDF = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"com.facebook.presto.jdbc.PrestoDriver\") \\\n",
    "    .option(\"url\", \"jdbc:presto://presto:8080/hive/default\") \\\n",
    "    .option(\"user\", \"hive\") \\\n",
    "    .option(\"dbtable\", \"pessoas\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
